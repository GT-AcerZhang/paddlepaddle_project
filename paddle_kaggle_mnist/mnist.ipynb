{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#引用需要的库和函数\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import paddle.fluid.dygraph as dygraph\n",
    "from paddle.fluid.dygraph.nn import Conv2D, Pool2D, BatchNorm, Linear\n",
    "from paddle.fluid.dygraph.base import to_variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na = load_dataset()\\nnums = 0\\nfor i,set in enumerate(a):\\n    nums = nums+1\\nx,y = set\\nprint(nums)#看看总minibatch个数是不是 42000/batch_size 大小\\nprint(x.shape[0],x.shape[1],x.shape[2],x.shape[3])#输出最后一个minibatch维度看看\\nprint(y.shape[0],y.shape[1])#输出最后一个minibatch维度看看\\n'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取数据\n",
    "def load_dataset(batch_size = 100):\n",
    "    #获取训练集\n",
    "    df = pd.read_csv('train.csv')  # 得到的是一个字典集\n",
    "    f1 = [f\"pixel{i}\" for i in range(0, 28 * 28)]  # 产生字符串列表，从pixel0到pixel783\n",
    "    f2 = 'label'\n",
    "    train_x = np.array(df[f1].values)  # 通过键获取字典数据，并且转化为矩阵\n",
    "    train_y = np.array(df[f2].values)\n",
    "    \n",
    "    '''\n",
    "    train_y=pd.Series(train_y)\n",
    "    train_y=np.array(pd.get_dummies(train_y))#独热码实现softmax\n",
    "    '''\n",
    "    #print(train_y[0:12])#查看一部分标签内容\n",
    "    #print(train_x.shape[0], train_x.shape[1])  # 输出训练集特征x维度,为（42000，784）\n",
    "    #print(train_y.shape[0])  # 输出标签y维度（42000，）\n",
    "    \n",
    "    #将数据处理成（样本数，通道数，长，宽）形式,像素化为0-1之间的数\n",
    "    train_x = np.reshape(train_x,[train_x.shape[0],1,28,28]).astype(np.float32) /255\n",
    "    train_y = np.reshape(train_y,[train_y.shape[0],1]).astype(np.int64)\n",
    "    \n",
    "    #定义数据读取器\n",
    "    def data_generator():\n",
    "        data_lists = []#存储minibatch的训练数据像素\n",
    "        label_list = []#存储minibatch的训练数据标签\n",
    "        \n",
    "        for id,data in enumerate(train_x):\n",
    "            data_lists.append(data)\n",
    "            label_list.append(train_y[id])\n",
    "            \n",
    "            if len(data_lists) == batch_size:#每minibatch存储一次，batch_size设置成100大小\n",
    "                yield np.array(data_lists),np.array(label_list)\n",
    "                data_lists = []#清空存储器\n",
    "                label_list = []\n",
    "        if len(data_lists) > 0:#其余的作为一组数据\n",
    "            yield np.array(data_lists),np.array(label_list)\n",
    "    \n",
    "    return data_generator()\n",
    "\n",
    "def load_test(batch_size = 100):\n",
    "    #获取测试集\n",
    "    dp = pd.read_csv('test.csv')  # 得到的是一个字典集\n",
    "    f = [f\"pixel{i}\" for i in range(0, 28 * 28)]  # 产生字符串列表，从pixel0到pixel783\n",
    "    test_x = np.array(dp[f].values)  # 通过键获取字典数据，并且转化为矩阵\n",
    "    #print(test_x.shape[0], test_x.shape[1])  # 输出维度，为（28000，784）\n",
    "    \n",
    "    #将数据处理成（样本数，通道数，长，宽）形式,像素化为0-1之间的数\n",
    "    test_x = np.reshape(test_x,[test_x.shape[0],1,28,28]).astype(np.float32) /255\n",
    "    \n",
    "    #定义数据读取器\n",
    "    def data_generator():\n",
    "        data_lists = []#存储minibatch的训练数据像素\n",
    "        \n",
    "        for id,data in enumerate(test_x):\n",
    "            data_lists.append(data)\n",
    "            \n",
    "            if len(data_lists) == batch_size:#每minibatch存储一次，batch_size设置成100大小\n",
    "                yield np.array(data_lists)\n",
    "                data_lists = []#清空存储器\n",
    "                \n",
    "        if len(data_lists) > 0:#其余的作为一组数据\n",
    "            yield np.array(data_lists)\n",
    "    \n",
    "    return data_generator()\n",
    "\"\"\"\n",
    "a = load_dataset()\n",
    "nums = 0\n",
    "for i,set in enumerate(a):\n",
    "    nums = nums+1\n",
    "x,y = set\n",
    "print(nums)#看看总minibatch个数是不是 42000/batch_size 大小\n",
    "print(x.shape[0],x.shape[1],x.shape[2],x.shape[3])#输出最后一个minibatch维度看看\n",
    "print(y.shape[0],y.shape[1])#输出最后一个minibatch维度看看\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 LeNet 网络结构\n",
    "class LeNet(fluid.dygraph.Layer):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # 创建卷积和池化层块，每个卷积层使用Sigmoid激活函数，后面跟着一个2x2的池化\n",
    "        self.conv1 = Conv2D(num_channels=1, num_filters=6, filter_size=5, act='sigmoid')\n",
    "        self.pool1 = Pool2D(pool_size=2, pool_stride=2, pool_type='max')\n",
    "        self.conv2 = Conv2D(num_channels=6, num_filters=16, filter_size=5, act='sigmoid')\n",
    "        self.pool2 = Pool2D(pool_size=2, pool_stride=2, pool_type='max')\n",
    "        # 创建第3个卷积层\n",
    "        self.conv3 = Conv2D(num_channels=16, num_filters=120, filter_size=4, act='sigmoid')\n",
    "        # 创建全连接层，第一个全连接层的输出神经元个数为64， 第二个全连接层输出神经元个数为分类标签的类别数\n",
    "        self.fc1 = Linear(input_dim=120, output_dim=64, act='sigmoid')\n",
    "        self.fc2 = Linear(input_dim=64, output_dim=num_classes)\n",
    "    # 网络的前向计算过程\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = fluid.layers.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 AlexNet 网络结构\n",
    "class AlexNet(fluid.dygraph.Layer):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # AlexNet与LeNet一样也会同时使用卷积和池化层提取图像特征\n",
    "        # 与LeNet不同的是激活函数换成了‘relu’\n",
    "        #这里将conv1中的输入通道数设置成1，输入为（样本数m，1，28，28）\n",
    "        self.conv1 = Conv2D(num_channels=1, num_filters=10, filter_size=11, stride=1, padding=3, act='relu')\n",
    "        #输出为（m,10，24，24）\n",
    "        self.pool1 = Pool2D(pool_size=2, pool_stride=2, pool_type='max')\n",
    "        #输出为（m,10,12,12）\n",
    "        self.conv2 = Conv2D(num_channels=10, num_filters=100, filter_size=5, stride=1, padding=2, act='relu')\n",
    "        #（m,100,12,12）\n",
    "        self.pool2 = Pool2D(pool_size=2, pool_stride=2, pool_type='max')\n",
    "        #(m,100,6,6)\n",
    "        self.conv3 = Conv2D(num_channels=100, num_filters=200, filter_size=3, stride=1, padding=0, act='relu')\n",
    "        #(m,200,4,4)\n",
    "        self.conv4 = Conv2D(num_channels=200, num_filters=200, filter_size=3, stride=1, padding=1, act='relu')\n",
    "        #(m,200,4,4)\n",
    "        self.conv5 = Conv2D(num_channels=200, num_filters=100, filter_size=3, stride=1, padding=1, act='relu')\n",
    "        #(m,100,4,4)\n",
    "        self.pool5 = Pool2D(pool_size=2, pool_stride=2, pool_type='max')#相当于无效\n",
    "        #(m,100,2,2)\n",
    "        self.fc1 = Linear(input_dim=400, output_dim=64, act='relu')\n",
    "        self.drop_ratio1 = 0.5\n",
    "        self.fc2 = Linear(input_dim=64, output_dim=64, act='relu')\n",
    "        self.drop_ratio2 = 0.5\n",
    "        self.fc3 = Linear(input_dim=64, output_dim=num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.pool5(x)\n",
    "        x = fluid.layers.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc1(x)\n",
    "        # 在全连接之后使用dropout抑制过拟合\n",
    "        x= fluid.layers.dropout(x, self.drop_ratio1)\n",
    "        x = self.fc2(x)\n",
    "        # 在全连接之后使用dropout抑制过拟合\n",
    "        x = fluid.layers.dropout(x, self.drop_ratio2)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ... \n",
      "epoch: 0, batch_id: 0, loss is: [3.0867326]\n",
      "epoch: 0, batch_id: 1000, loss is: [2.331239]\n",
      "epoch: 0, batch_id: 2000, loss is: [2.2960057]\n",
      "epoch: 0, batch_id: 3000, loss is: [1.0199381]\n",
      "epoch: 0, batch_id: 4000, loss is: [0.61462504]\n",
      "[train] accuracy/loss: 0.8828095197677612/0.3804200291633606\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义训练过程\n",
    "def train(model):\n",
    "    print('start training ... ')\n",
    "    model.train()\n",
    "    epoch_num = 10\n",
    "    batch_size = 10\n",
    "    # 定义学习率，并加载优化器参数到模型中\n",
    "    total_steps = (int(42000//batch_size) + 1) * epoch_num\n",
    "    lr = fluid.dygraph.PolynomialDecay(0.01, total_steps, 0.001)\n",
    "    \n",
    "    opt = fluid.optimizer.Momentum(learning_rate=lr, momentum=0.9, parameter_list=model.parameters())\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        for batch_id, data in enumerate(load_dataset(batch_size)):\n",
    "            # 读入数据\n",
    "            x_data,y_data = data\n",
    "            # 将numpy.ndarray转化成Tensor\n",
    "            img = fluid.dygraph.to_variable(x_data)\n",
    "            label = fluid.dygraph.to_variable(y_data)\n",
    "            # 计算模型输出\n",
    "            logits = model(img)\n",
    "            # 计算损失函数\n",
    "            loss = fluid.layers.softmax_with_cross_entropy(logits, label)\n",
    "            avg_loss = fluid.layers.mean(loss)\n",
    "            if batch_id % 1000 == 0:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, avg_loss.numpy()))\n",
    "            #更新梯度\n",
    "            avg_loss.backward()\n",
    "            opt.minimize(avg_loss)\n",
    "            #清除梯度\n",
    "            model.clear_gradients()    \n",
    "            \n",
    "            \n",
    "        #在训练集上的评估结果\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(load_dataset(batch_size)):\n",
    "            # 调整输入数据形状和类型\n",
    "            x_data,y_data = data\n",
    "            # 将numpy.ndarray转化成Tensor\n",
    "            img = fluid.dygraph.to_variable(x_data)\n",
    "            label = fluid.dygraph.to_variable(y_data)\n",
    "            # 计算模型输出\n",
    "            logits = model(img)\n",
    "            pred = fluid.layers.softmax(logits)\n",
    "            # 计算损失函数\n",
    "            loss = fluid.layers.softmax_with_cross_entropy(logits, label)\n",
    "            acc = fluid.layers.accuracy(pred, label)\n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        print(\"[train] accuracy/loss: {}/{}\".format(np.mean(accuracies), np.mean(losses)))\n",
    "        model.train()\n",
    "    \n",
    "    #预测结果:\n",
    "    model.eval()\n",
    "    #获取测试集\n",
    "    #将结果写入LeNet.csv文件\n",
    "    lenet = open(\"LeNet1.csv\", \"w\")\n",
    "    lenet.write(\"ImageId,Label\\n\")\n",
    "    for id,test_data in enumerate(load_test(batch_size=1000)):\n",
    "        # 将numpy.ndarray转化成Tensor\n",
    "        img = fluid.dygraph.to_variable(test_data)\n",
    "        # 计算模型输出\n",
    "        logits = model(img)\n",
    "        #输出softmax层结果，得到图片分类\n",
    "        predict = fluid.layers.softmax(logits).numpy()\n",
    "        #将预测结果最大值下标作为分类结果对应0-9\n",
    "\n",
    "        label = []\n",
    "        for i in range(predict.shape[0]):\n",
    "            label.append([i+1,np.argmax(predict[i])])\n",
    "\n",
    "        for pred in label:\n",
    "            id,y = pred[0],pred[1]\n",
    "            lenet.write(str(id) + \",\" + str(y) + \"\\n\")\n",
    "    # 保存模型参数\n",
    "    fluid.save_dygraph(model.state_dict(), 'mnist_LeNet')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 创建模型\n",
    "    #是否使用gpu\n",
    "    use_gpu = True#确认使用gpu\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace\n",
    "    with fluid.dygraph.guard(place):\n",
    "        \n",
    "        model = LeNet(num_classes=10)\n",
    "        #启动训练过程\n",
    "        train(model)\n",
    " \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ... \n",
      "epoch: 0, batch_id: 0, loss is: [2.2860556]\n",
      "epoch: 0, batch_id: 1000, loss is: [1.2640915]\n",
      "epoch: 0, batch_id: 2000, loss is: [0.5055747]\n",
      "epoch: 0, batch_id: 3000, loss is: [0.1636792]\n",
      "epoch: 0, batch_id: 4000, loss is: [0.62262416]\n",
      "[train] accuracy/loss: 0.955880880355835/0.1565110683441162\n",
      "epoch: 1, batch_id: 0, loss is: [0.20113522]\n",
      "epoch: 1, batch_id: 1000, loss is: [0.10807893]\n",
      "epoch: 1, batch_id: 2000, loss is: [0.02898502]\n",
      "epoch: 1, batch_id: 3000, loss is: [0.0464281]\n",
      "epoch: 1, batch_id: 4000, loss is: [0.22280589]\n",
      "[train] accuracy/loss: 0.9745714068412781/0.09044940024614334\n",
      "epoch: 2, batch_id: 0, loss is: [0.12075842]\n",
      "epoch: 2, batch_id: 1000, loss is: [0.21634105]\n",
      "epoch: 2, batch_id: 2000, loss is: [0.06711139]\n",
      "epoch: 2, batch_id: 3000, loss is: [0.06434724]\n",
      "epoch: 2, batch_id: 4000, loss is: [0.2261297]\n",
      "[train] accuracy/loss: 0.979285717010498/0.07005864381790161\n",
      "epoch: 3, batch_id: 0, loss is: [0.20187762]\n",
      "epoch: 3, batch_id: 1000, loss is: [0.06105394]\n",
      "epoch: 3, batch_id: 2000, loss is: [0.01145389]\n",
      "epoch: 3, batch_id: 3000, loss is: [0.29760024]\n",
      "epoch: 3, batch_id: 4000, loss is: [0.08719968]\n",
      "[train] accuracy/loss: 0.9863333702087402/0.047155119478702545\n",
      "epoch: 4, batch_id: 0, loss is: [0.00893742]\n",
      "epoch: 4, batch_id: 1000, loss is: [0.18580827]\n",
      "epoch: 4, batch_id: 2000, loss is: [0.12746209]\n",
      "epoch: 4, batch_id: 3000, loss is: [0.06660718]\n",
      "epoch: 4, batch_id: 4000, loss is: [0.3799512]\n",
      "[train] accuracy/loss: 0.9908095002174377/0.03400696814060211\n",
      "epoch: 5, batch_id: 0, loss is: [0.011305]\n",
      "epoch: 5, batch_id: 1000, loss is: [0.11344434]\n",
      "epoch: 5, batch_id: 2000, loss is: [0.00617507]\n",
      "epoch: 5, batch_id: 3000, loss is: [0.03170862]\n",
      "epoch: 5, batch_id: 4000, loss is: [0.28445512]\n",
      "[train] accuracy/loss: 0.9921665787696838/0.02838512323796749\n",
      "epoch: 6, batch_id: 0, loss is: [0.00032582]\n",
      "epoch: 6, batch_id: 1000, loss is: [0.00024951]\n",
      "epoch: 6, batch_id: 2000, loss is: [0.0838112]\n",
      "epoch: 6, batch_id: 3000, loss is: [0.02323825]\n",
      "epoch: 6, batch_id: 4000, loss is: [0.02739505]\n",
      "[train] accuracy/loss: 0.9911666512489319/0.03209998086094856\n",
      "epoch: 7, batch_id: 0, loss is: [0.00288637]\n",
      "epoch: 7, batch_id: 1000, loss is: [0.07710113]\n",
      "epoch: 7, batch_id: 2000, loss is: [0.00019715]\n",
      "epoch: 7, batch_id: 3000, loss is: [0.00208662]\n",
      "epoch: 7, batch_id: 4000, loss is: [0.01424045]\n",
      "[train] accuracy/loss: 0.9936904907226562/0.023274198174476624\n",
      "epoch: 8, batch_id: 0, loss is: [0.00183445]\n",
      "epoch: 8, batch_id: 1000, loss is: [0.01460401]\n",
      "epoch: 8, batch_id: 2000, loss is: [0.33497143]\n",
      "epoch: 8, batch_id: 3000, loss is: [0.00059182]\n",
      "epoch: 8, batch_id: 4000, loss is: [0.00290344]\n",
      "[train] accuracy/loss: 0.9959523677825928/0.01544656977057457\n",
      "epoch: 9, batch_id: 0, loss is: [0.06536549]\n",
      "epoch: 9, batch_id: 1000, loss is: [0.0032749]\n",
      "epoch: 9, batch_id: 2000, loss is: [0.00801221]\n",
      "epoch: 9, batch_id: 3000, loss is: [0.00816097]\n",
      "epoch: 9, batch_id: 4000, loss is: [0.00062171]\n",
      "[train] accuracy/loss: 0.9965476393699646/0.013189496472477913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义训练过程\n",
    "def train(model):\n",
    "    print('start training ... ')\n",
    "    model.train()\n",
    "    epoch_num = 10\n",
    "    batch_size = 10\n",
    "    # 定义学习率，并加载优化器参数到模型中\n",
    "    total_steps = (int(42000//batch_size) + 1) * epoch_num\n",
    "    lr = fluid.dygraph.PolynomialDecay(0.001, total_steps, 0.0001)\n",
    "    \n",
    "    opt = fluid.optimizer.Momentum(learning_rate=lr, momentum=0.9, parameter_list=model.parameters())\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        for batch_id, data in enumerate(load_dataset(batch_size)):\n",
    "            # 读入数据\n",
    "            x_data,y_data = data\n",
    "            # 将numpy.ndarray转化成Tensor\n",
    "            img = fluid.dygraph.to_variable(x_data)\n",
    "            label = fluid.dygraph.to_variable(y_data)\n",
    "            # 计算模型输出\n",
    "            logits = model(img)\n",
    "            # 计算损失函数\n",
    "            loss = fluid.layers.softmax_with_cross_entropy(logits, label)\n",
    "            avg_loss = fluid.layers.mean(loss)\n",
    "            if batch_id % 1000 == 0:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, avg_loss.numpy()))\n",
    "            #更新梯度\n",
    "            avg_loss.backward()\n",
    "            opt.minimize(avg_loss)\n",
    "            #清除梯度\n",
    "            model.clear_gradients()    \n",
    "            \n",
    "            \n",
    "        #在训练集上的评估结果\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(load_dataset(batch_size)):\n",
    "            # 调整输入数据形状和类型\n",
    "            x_data,y_data = data\n",
    "            \n",
    "            \n",
    "            # 将numpy.ndarray转化成Tensor\n",
    "            img = fluid.dygraph.to_variable(x_data)\n",
    "            label = fluid.dygraph.to_variable(y_data)\n",
    "            # 计算模型输出\n",
    "            logits = model(img)\n",
    "            pred = fluid.layers.softmax(logits)\n",
    "            # 计算损失函数\n",
    "            loss = fluid.layers.softmax_with_cross_entropy(logits, label)\n",
    "            acc = fluid.layers.accuracy(pred, label)\n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        print(\"[train] accuracy/loss: {}/{}\".format(np.mean(accuracies), np.mean(losses)))\n",
    "        model.train()\n",
    "    \n",
    "    #预测结果:\n",
    "    model.eval()\n",
    "    #获取测试集\n",
    "    #将结果写入LeNet.csv文件\n",
    "    lenet = open(\"AlexNet.csv\", \"w\")\n",
    "    lenet.write(\"ImageId,Label\\n\")\n",
    "    for id,test_data in enumerate(load_test(batch_size=1000)):\n",
    "        # 将numpy.ndarray转化成Tensor\n",
    "        img = fluid.dygraph.to_variable(test_data)\n",
    "        # 计算模型输出\n",
    "        logits = model(img)\n",
    "        #输出softmax层结果，得到图片分类\n",
    "        predict = fluid.layers.softmax(logits).numpy()\n",
    "        #将预测结果最大值下标作为分类结果对应0-9\n",
    "\n",
    "        label = []\n",
    "        for i in range(predict.shape[0]):\n",
    "            label.append([i+1,np.argmax(predict[i])])\n",
    "\n",
    "        for pred in label:\n",
    "            id,y = pred[0],pred[1]\n",
    "            lenet.write(str(id) + \",\" + str(y) + \"\\n\")\n",
    "    # 保存模型参数\n",
    "    fluid.save_dygraph(model.state_dict(), 'mnist_LeNet')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 创建模型\n",
    "    #是否使用gpu\n",
    "    use_gpu = True#确认使用gpu\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace\n",
    "    with fluid.dygraph.guard(place):\n",
    "        \"\"\"\n",
    "        model = LeNet(num_classes=10)\n",
    "        #启动训练过程\n",
    "        train(model)\n",
    " \n",
    "        \"\"\"\n",
    "        model = AlexNet(num_classes=10)\n",
    "        train(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
